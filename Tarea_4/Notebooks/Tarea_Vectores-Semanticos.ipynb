{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🟥 Ejercicios Tarea\n",
    "## **Tarea de clasificación con el corpus `20newsgroups`**. Probar las siguientes estrategias y en cada caso medir el F1 score:\n",
    "\n",
    "### (En cada uno de los modelos BOW/TF-IDF que construyas puedes ajustar el hiperparámetro `max_features`)\n",
    "\n",
    "### 1. Todas las clases, sin quitar *headers*, *quotes*, *footers*. Comparar:\n",
    " #### **BOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el corpus de 20newsgroups con todas las clases y sin quitar headers, quotes, footers\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=())\n",
    "\n",
    "# Separar los datos en características y labels\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       108\n",
      "           1       0.59      0.58      0.59       148\n",
      "           2       0.68      0.72      0.70       156\n",
      "           3       0.47      0.52      0.49       136\n",
      "           4       0.70      0.63      0.66       160\n",
      "           5       0.70      0.68      0.69       168\n",
      "           6       0.75      0.69      0.72       149\n",
      "           7       0.69      0.75      0.72       149\n",
      "           8       0.82      0.79      0.80       127\n",
      "           9       0.78      0.81      0.79       160\n",
      "          10       0.84      0.83      0.84       145\n",
      "          11       0.86      0.83      0.84       152\n",
      "          12       0.66      0.65      0.66       152\n",
      "          13       0.74      0.74      0.74       151\n",
      "          14       0.80      0.85      0.82       126\n",
      "          15       0.80      0.88      0.84       147\n",
      "          16       0.80      0.77      0.79       140\n",
      "          17       0.87      0.83      0.85       131\n",
      "          18       0.66      0.72      0.69       120\n",
      "          19       0.65      0.57      0.61       102\n",
      "\n",
      "    accuracy                           0.73      2827\n",
      "   macro avg       0.73      0.73      0.73      2827\n",
      "weighted avg       0.73      0.73      0.73      2827\n",
      "\n",
      "F1 score weighted: 0.7287243263382336\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')  # Se puede ajustar max_features según sea necesario\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Entrenar modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predecir con el conjunto de prueba\n",
    "y_pred = lr.predict(X_test_bow)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "#print(f\"F1 score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **TF-IDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.73       108\n",
      "           1       0.63      0.65      0.64       148\n",
      "           2       0.74      0.75      0.74       156\n",
      "           3       0.54      0.58      0.56       136\n",
      "           4       0.73      0.65      0.69       160\n",
      "           5       0.72      0.71      0.72       168\n",
      "           6       0.78      0.69      0.73       149\n",
      "           7       0.74      0.81      0.77       149\n",
      "           8       0.88      0.84      0.86       127\n",
      "           9       0.80      0.82      0.81       160\n",
      "          10       0.86      0.84      0.85       145\n",
      "          11       0.92      0.85      0.88       152\n",
      "          12       0.63      0.69      0.66       152\n",
      "          13       0.77      0.83      0.80       151\n",
      "          14       0.78      0.84      0.81       126\n",
      "          15       0.79      0.88      0.83       147\n",
      "          16       0.86      0.85      0.85       140\n",
      "          17       0.93      0.87      0.90       131\n",
      "          18       0.73      0.71      0.72       120\n",
      "          19       0.67      0.50      0.57       102\n",
      "\n",
      "    accuracy                           0.76      2827\n",
      "   macro avg       0.76      0.76      0.76      2827\n",
      "weighted avg       0.76      0.76      0.76      2827\n",
      "\n",
      "F1 score weighted: 0.758441318115043\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Entrenar modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predecir con el conjunto de prueba\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BOW + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train (BOW): (16019, 1000)\n",
      "Dimensiones de X_test (BOW): (2827, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Dimensiones de los datos BOW\n",
    "print(f\"Dimensiones de X_train (BOW): {X_train_bow.shape}\")\n",
    "print(f\"Dimensiones de X_test (BOW): {X_test_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997812962458167\n"
     ]
    }
   ],
   "source": [
    "#¿Cuántos componentes principales utilizar?\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(X_train_bow.toarray())\n",
    "print(sum(pca.explained_variance_ratio_))  # Porcentaje de la varianza total retenida por los componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       108\n",
      "           1       0.58      0.58      0.58       148\n",
      "           2       0.70      0.72      0.71       156\n",
      "           3       0.48      0.50      0.49       136\n",
      "           4       0.71      0.62      0.67       160\n",
      "           5       0.72      0.69      0.71       168\n",
      "           6       0.70      0.67      0.69       149\n",
      "           7       0.70      0.74      0.72       149\n",
      "           8       0.85      0.78      0.81       127\n",
      "           9       0.76      0.81      0.78       160\n",
      "          10       0.81      0.81      0.81       145\n",
      "          11       0.87      0.80      0.84       152\n",
      "          12       0.57      0.60      0.59       152\n",
      "          13       0.67      0.72      0.69       151\n",
      "          14       0.72      0.79      0.75       126\n",
      "          15       0.79      0.78      0.79       147\n",
      "          16       0.74      0.74      0.74       140\n",
      "          17       0.86      0.84      0.85       131\n",
      "          18       0.64      0.66      0.65       120\n",
      "          19       0.60      0.54      0.57       102\n",
      "\n",
      "    accuracy                           0.71      2827\n",
      "   macro avg       0.71      0.70      0.71      2827\n",
      "weighted avg       0.71      0.71      0.71      2827\n",
      "\n",
      "F1 score weighted: 0.7071456413162504\n"
     ]
    }
   ],
   "source": [
    "# Aplicar PCA sobre los datos BOW\n",
    "pca = PCA(n_components=300)  # Puedes ajustar n_components para reducir más o menos dimensiones\n",
    "X_train_bow_pca = pca.fit_transform(X_train_bow.toarray())  # Convertir a array para PCA\n",
    "X_test_bow_pca = pca.transform(X_test_bow.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por PCA\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_pca, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con BOW + PCA\n",
    "y_pred = lr.predict(X_test_bow_pca)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **TF-IDF + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train (TF-IDF): (16019, 1000)\n",
      "Dimensiones de X_test (TF-IDF): (2827, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Dimensiones de los datos TF-IDF\n",
    "print(f\"Dimensiones de X_train (TF-IDF): {X_train_tfidf.shape}\")\n",
    "print(f\"Dimensiones de X_test (TF-IDF): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48026393135871365\n"
     ]
    }
   ],
   "source": [
    "#¿Cuántos componentes principales utilizar?\n",
    "pca = PCA(n_components=200)\n",
    "pca.fit(X_train_tfidf.toarray())\n",
    "print(sum(pca.explained_variance_ratio_))  # Porcentaje de la varianza total retenida por los componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.74      0.69       108\n",
      "           1       0.60      0.60      0.60       148\n",
      "           2       0.69      0.69      0.69       156\n",
      "           3       0.53      0.57      0.55       136\n",
      "           4       0.70      0.61      0.65       160\n",
      "           5       0.68      0.65      0.66       168\n",
      "           6       0.72      0.67      0.69       149\n",
      "           7       0.74      0.76      0.75       149\n",
      "           8       0.79      0.78      0.79       127\n",
      "           9       0.79      0.82      0.80       160\n",
      "          10       0.81      0.81      0.81       145\n",
      "          11       0.91      0.80      0.85       152\n",
      "          12       0.52      0.61      0.56       152\n",
      "          13       0.66      0.75      0.70       151\n",
      "          14       0.76      0.81      0.78       126\n",
      "          15       0.76      0.85      0.80       147\n",
      "          16       0.81      0.80      0.81       140\n",
      "          17       0.90      0.86      0.88       131\n",
      "          18       0.67      0.68      0.67       120\n",
      "          19       0.65      0.36      0.47       102\n",
      "\n",
      "    accuracy                           0.71      2827\n",
      "   macro avg       0.72      0.71      0.71      2827\n",
      "weighted avg       0.72      0.71      0.71      2827\n",
      "\n",
      "F1 score weighted: 0.7138955012071461\n"
     ]
    }
   ],
   "source": [
    "# Aplicar PCA sobre los datos TF-IDF\n",
    "pca = PCA(n_components=200)  # Ajustar n_components para reducir a las dimensiones que prefieras\n",
    "X_train_tfidf_pca = pca.fit_transform(X_train_tfidf.toarray())  # Convertir a array para PCA\n",
    "X_test_tfidf_pca = pca.transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por PCA\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_pca, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con TF-IDF + PCA\n",
    "y_pred = lr.predict(X_test_tfidf_pca)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **BOW + t-SNE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       108\n",
      "           1       0.15      0.04      0.06       148\n",
      "           2       0.05      0.01      0.01       156\n",
      "           3       0.04      0.04      0.04       136\n",
      "           4       0.00      0.00      0.00       160\n",
      "           5       0.06      0.04      0.05       168\n",
      "           6       0.00      0.00      0.00       149\n",
      "           7       0.05      0.06      0.05       149\n",
      "           8       0.04      0.17      0.06       127\n",
      "           9       0.05      0.09      0.06       160\n",
      "          10       0.08      0.26      0.13       145\n",
      "          11       0.03      0.03      0.03       152\n",
      "          12       0.06      0.04      0.05       152\n",
      "          13       0.09      0.14      0.11       151\n",
      "          14       0.16      0.24      0.19       126\n",
      "          15       0.15      0.25      0.18       147\n",
      "          16       0.00      0.00      0.00       140\n",
      "          17       0.00      0.00      0.00       131\n",
      "          18       0.00      0.00      0.00       120\n",
      "          19       0.00      0.00      0.00       102\n",
      "\n",
      "    accuracy                           0.07      2827\n",
      "   macro avg       0.05      0.07      0.05      2827\n",
      "weighted avg       0.05      0.07      0.05      2827\n",
      "\n",
      "F1 score weighted: 0.052286680280751834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Aplicar t-SNE sobre los datos BOW\n",
    "tsne = TSNE(n_components=2, random_state=42) \n",
    "X_train_bow_tsne = tsne.fit_transform(X_train_bow.toarray())  # Convertir a array para t-SNE\n",
    "X_test_bow_tsne = tsne.fit_transform(X_test_bow.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_tsne, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con BOW + t-SNE\n",
    "y_pred = lr.predict(X_test_bow_tsne)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con n_components=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.03      0.04       108\n",
      "           1       0.05      0.04      0.04       148\n",
      "           2       0.03      0.01      0.01       156\n",
      "           3       0.06      0.01      0.01       136\n",
      "           4       0.03      0.01      0.01       160\n",
      "           5       0.02      0.02      0.02       168\n",
      "           6       0.00      0.00      0.00       149\n",
      "           7       0.00      0.00      0.00       149\n",
      "           8       0.02      0.05      0.03       127\n",
      "           9       0.02      0.03      0.02       160\n",
      "          10       0.04      0.09      0.06       145\n",
      "          11       0.02      0.05      0.03       152\n",
      "          12       0.05      0.02      0.03       152\n",
      "          13       0.06      0.01      0.02       151\n",
      "          14       0.08      0.23      0.12       126\n",
      "          15       0.00      0.01      0.00       147\n",
      "          16       0.04      0.08      0.05       140\n",
      "          17       0.01      0.02      0.02       131\n",
      "          18       0.00      0.00      0.00       120\n",
      "          19       0.00      0.00      0.00       102\n",
      "\n",
      "    accuracy                           0.03      2827\n",
      "   macro avg       0.03      0.03      0.03      2827\n",
      "weighted avg       0.03      0.03      0.03      2827\n",
      "\n",
      "F1 score weighted: 0.025612876317471226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Aplicar t-SNE sobre los datos BOW\n",
    "tsne = TSNE(n_components=3, random_state=42) \n",
    "X_train_bow_tsne = tsne.fit_transform(X_train_bow.toarray())  # Convertir a array para t-SNE\n",
    "X_test_bow_tsne = tsne.fit_transform(X_test_bow.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_tsne, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con BOW + t-SNE\n",
    "y_pred = lr.predict(X_test_bow_tsne)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF + t-SNE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       108\n",
      "           1       0.03      0.02      0.02       148\n",
      "           2       0.00      0.00      0.00       156\n",
      "           3       0.00      0.00      0.00       136\n",
      "           4       0.00      0.00      0.00       160\n",
      "           5       0.00      0.00      0.00       168\n",
      "           6       0.01      0.01      0.01       149\n",
      "           7       0.02      0.04      0.03       149\n",
      "           8       0.20      0.25      0.22       127\n",
      "           9       0.06      0.01      0.02       160\n",
      "          10       0.04      0.12      0.06       145\n",
      "          11       0.13      0.17      0.15       152\n",
      "          12       0.08      0.09      0.08       152\n",
      "          13       0.04      0.06      0.05       151\n",
      "          14       0.24      0.44      0.31       126\n",
      "          15       0.00      0.00      0.00       147\n",
      "          16       0.01      0.01      0.01       140\n",
      "          17       0.00      0.00      0.00       131\n",
      "          18       0.00      0.00      0.00       120\n",
      "          19       0.00      0.00      0.00       102\n",
      "\n",
      "    accuracy                           0.06      2827\n",
      "   macro avg       0.04      0.06      0.05      2827\n",
      "weighted avg       0.04      0.06      0.05      2827\n",
      "\n",
      "F1 score weighted: 0.046862518732083885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Importar la librería para t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Aplicar t-SNE sobre los datos TF-IDF\n",
    "tsne = TSNE(n_components=2, random_state=42) \n",
    "X_train_tfidf_tsne = tsne.fit_transform(X_train_tfidf.toarray())  # Convertir a array para t-SNE\n",
    "X_test_tfidf_tsne = tsne.fit_transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_tsne, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con TF-IDF + t-SNE\n",
    "y_pred = lr.predict(X_test_tfidf_tsne)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con n_components=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       108\n",
      "           1       0.04      0.01      0.01       148\n",
      "           2       0.00      0.00      0.00       156\n",
      "           3       0.00      0.00      0.00       136\n",
      "           4       0.00      0.00      0.00       160\n",
      "           5       0.06      0.07      0.06       168\n",
      "           6       0.01      0.01      0.01       149\n",
      "           7       0.10      0.19      0.13       149\n",
      "           8       0.33      0.31      0.32       127\n",
      "           9       0.07      0.01      0.01       160\n",
      "          10       0.07      0.17      0.10       145\n",
      "          11       0.06      0.05      0.06       152\n",
      "          12       0.24      0.03      0.06       152\n",
      "          13       0.03      0.01      0.02       151\n",
      "          14       0.03      0.09      0.05       126\n",
      "          15       0.00      0.01      0.01       147\n",
      "          16       0.01      0.02      0.02       140\n",
      "          17       0.00      0.00      0.00       131\n",
      "          18       0.00      0.00      0.00       120\n",
      "          19       0.00      0.00      0.00       102\n",
      "\n",
      "    accuracy                           0.05      2827\n",
      "   macro avg       0.05      0.05      0.04      2827\n",
      "weighted avg       0.05      0.05      0.04      2827\n",
      "\n",
      "F1 score weighted: 0.0420826980533869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Aplicar t-SNE sobre los datos TF-IDF\n",
    "tsne = TSNE(n_components=3, random_state=42) \n",
    "X_train_tfidf_tsne = tsne.fit_transform(X_train_tfidf.toarray())  # Convertir a array para t-SNE\n",
    "X_test_tfidf_tsne = tsne.fit_transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_tsne, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba con TF-IDF + t-SNE\n",
    "y_pred = lr.predict(X_test_tfidf_tsne)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Las mismas 6 estrategias del paso anterior, quitando *headers*, *quotes*, *footers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el corpus de 20newsgroups quitando headers, quotes, y footers\n",
    "newsgroups_clean = fetch_20newsgroups(subset='all', remove=('headers', 'quotes', 'footers'))\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X_clean = newsgroups_clean.data\n",
    "y_clean = newsgroups_clean.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **BOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.41      0.40       236\n",
      "           1       0.52      0.47      0.50       287\n",
      "           2       0.55      0.53      0.54       290\n",
      "           3       0.47      0.46      0.47       285\n",
      "           4       0.53      0.47      0.50       312\n",
      "           5       0.65      0.59      0.62       308\n",
      "           6       0.62      0.62      0.62       276\n",
      "           7       0.52      0.47      0.49       304\n",
      "           8       0.28      0.57      0.37       279\n",
      "           9       0.54      0.54      0.54       308\n",
      "          10       0.72      0.67      0.69       309\n",
      "          11       0.63      0.62      0.62       290\n",
      "          12       0.38      0.38      0.38       304\n",
      "          13       0.52      0.57      0.54       300\n",
      "          14       0.61      0.56      0.58       297\n",
      "          15       0.61      0.54      0.58       292\n",
      "          16       0.55      0.49      0.52       270\n",
      "          17       0.76      0.64      0.69       272\n",
      "          18       0.42      0.41      0.42       239\n",
      "          19       0.27      0.21      0.23       196\n",
      "\n",
      "    accuracy                           0.52      5654\n",
      "   macro avg       0.53      0.51      0.52      5654\n",
      "weighted avg       0.53      0.52      0.52      5654\n",
      "\n",
      "F1 score weighted: 0.5225726318021614\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_clean = CountVectorizer(max_features=1000, stop_words='english')  # Se puede ajustar max_features según sea necesario\n",
    "X_train_bow_clean = vectorizer_clean.fit_transform(X_train_clean)\n",
    "X_test_bow_clean = vectorizer_clean.transform(X_test_clean)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_clean = lr.predict(X_test_bow_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_clean, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **TF-IDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.45      0.44       236\n",
      "           1       0.60      0.53      0.56       287\n",
      "           2       0.60      0.59      0.59       290\n",
      "           3       0.50      0.54      0.52       285\n",
      "           4       0.60      0.52      0.56       312\n",
      "           5       0.68      0.63      0.66       308\n",
      "           6       0.66      0.65      0.66       276\n",
      "           7       0.61      0.52      0.56       304\n",
      "           8       0.31      0.57      0.41       279\n",
      "           9       0.59      0.56      0.57       308\n",
      "          10       0.75      0.70      0.72       309\n",
      "          11       0.69      0.59      0.64       290\n",
      "          12       0.42      0.47      0.44       304\n",
      "          13       0.49      0.66      0.56       300\n",
      "          14       0.67      0.60      0.63       297\n",
      "          15       0.60      0.66      0.63       292\n",
      "          16       0.58      0.58      0.58       270\n",
      "          17       0.77      0.64      0.70       272\n",
      "          18       0.49      0.48      0.48       239\n",
      "          19       0.29      0.11      0.16       196\n",
      "\n",
      "    accuracy                           0.56      5654\n",
      "   macro avg       0.57      0.55      0.55      5654\n",
      "weighted avg       0.57      0.56      0.56      5654\n",
      "\n",
      "F1 score weighted: 0.562248434957701\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones TF-IDF\n",
    "tfidf_vectorizer_clean = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf_clean = tfidf_vectorizer_clean.fit_transform(X_train_clean)\n",
    "X_test_tfidf_clean = tfidf_vectorizer_clean.transform(X_test_clean)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_clean = lr.predict(X_test_tfidf_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_clean, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BOW + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.40      0.40       236\n",
      "           1       0.55      0.47      0.51       287\n",
      "           2       0.54      0.51      0.53       290\n",
      "           3       0.46      0.48      0.47       285\n",
      "           4       0.58      0.49      0.53       312\n",
      "           5       0.65      0.60      0.63       308\n",
      "           6       0.64      0.63      0.64       276\n",
      "           7       0.52      0.51      0.52       304\n",
      "           8       0.27      0.59      0.37       279\n",
      "           9       0.51      0.55      0.53       308\n",
      "          10       0.69      0.65      0.67       309\n",
      "          11       0.67      0.61      0.64       290\n",
      "          12       0.41      0.38      0.40       304\n",
      "          13       0.49      0.57      0.53       300\n",
      "          14       0.64      0.57      0.60       297\n",
      "          15       0.62      0.56      0.59       292\n",
      "          16       0.55      0.50      0.52       270\n",
      "          17       0.76      0.62      0.69       272\n",
      "          18       0.46      0.41      0.44       239\n",
      "          19       0.34      0.23      0.28       196\n",
      "\n",
      "    accuracy                           0.52      5654\n",
      "   macro avg       0.54      0.52      0.52      5654\n",
      "weighted avg       0.54      0.52      0.53      5654\n",
      "\n",
      "F1 score weighted: 0.5295010140296195\n"
     ]
    }
   ],
   "source": [
    "# Aplicar PCA sobre los datos BOW ya limpios\n",
    "pca = PCA(n_components=300)  # Puedes ajustar n_components para reducir más o menos dimensiones\n",
    "X_train_bow_pca_clean = pca.fit_transform(X_train_bow_clean.toarray())  # Convertir a array para PCA\n",
    "X_test_bow_pca_clean = pca.transform(X_test_bow_clean.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por PCA\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_pca_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_pca_clean = lr.predict(X_test_bow_pca_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_pca_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_pca_clean, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **TF-IDF + PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.40      0.39       236\n",
      "           1       0.55      0.54      0.55       287\n",
      "           2       0.60      0.56      0.57       290\n",
      "           3       0.47      0.52      0.49       285\n",
      "           4       0.64      0.46      0.54       312\n",
      "           5       0.65      0.64      0.64       308\n",
      "           6       0.65      0.62      0.64       276\n",
      "           7       0.62      0.50      0.55       304\n",
      "           8       0.29      0.57      0.39       279\n",
      "           9       0.55      0.48      0.52       308\n",
      "          10       0.65      0.66      0.66       309\n",
      "          11       0.72      0.60      0.65       290\n",
      "          12       0.42      0.48      0.45       304\n",
      "          13       0.43      0.65      0.52       300\n",
      "          14       0.69      0.57      0.62       297\n",
      "          15       0.59      0.66      0.63       292\n",
      "          16       0.58      0.55      0.56       270\n",
      "          17       0.75      0.66      0.70       272\n",
      "          18       0.45      0.44      0.45       239\n",
      "          19       0.20      0.04      0.07       196\n",
      "\n",
      "    accuracy                           0.54      5654\n",
      "   macro avg       0.54      0.53      0.53      5654\n",
      "weighted avg       0.55      0.54      0.54      5654\n",
      "\n",
      "F1 score weighted: 0.5387268383523124\n"
     ]
    }
   ],
   "source": [
    "# Aplicar PCA sobre los datos TF-IDF ya limpios\n",
    "pca = PCA(n_components=200)  # Puedes ajustar n_components para reducir más o menos dimensiones\n",
    "X_train_tfidf_pca_clean = pca.fit_transform(X_train_tfidf_clean.toarray())  # Convertir a array para PCA\n",
    "X_test_tfidf_pca_clean = pca.transform(X_test_tfidf_clean.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por PCA\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_pca_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_tfidf_pca_clean = lr.predict(X_test_tfidf_pca_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_tfidf_pca_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_tfidf_pca_clean, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **BOW + t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       236\n",
      "           1       0.00      0.00      0.00       287\n",
      "           2       0.02      0.04      0.02       290\n",
      "           3       0.03      0.01      0.02       285\n",
      "           4       0.00      0.00      0.00       312\n",
      "           5       0.00      0.00      0.00       308\n",
      "           6       0.05      0.17      0.08       276\n",
      "           7       0.04      0.01      0.01       304\n",
      "           8       0.03      0.10      0.05       279\n",
      "           9       0.07      0.03      0.04       308\n",
      "          10       0.04      0.02      0.02       309\n",
      "          11       0.07      0.05      0.06       290\n",
      "          12       0.04      0.01      0.02       304\n",
      "          13       0.00      0.00      0.00       300\n",
      "          14       0.06      0.03      0.04       297\n",
      "          15       0.04      0.13      0.06       292\n",
      "          16       0.01      0.02      0.02       270\n",
      "          17       0.09      0.25      0.13       272\n",
      "          18       0.00      0.00      0.00       239\n",
      "          19       0.00      0.00      0.00       196\n",
      "\n",
      "    accuracy                           0.04      5654\n",
      "   macro avg       0.03      0.04      0.03      5654\n",
      "weighted avg       0.03      0.04      0.03      5654\n",
      "\n",
      "F1 score weighted: 0.028885601527581233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Aplicar t-SNE sobre los datos BOW ya limpios\n",
    "tsne = TSNE(n_components=2, random_state=42) \n",
    "X_train_bow_tsne_clean = tsne.fit_transform(X_train_bow_clean.toarray())  # Convertir a array para t-SNE\n",
    "X_test_bow_tsne_clean = tsne.fit_transform(X_test_bow_clean.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_tsne_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_tsne_clean = lr.predict(X_test_bow_tsne_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_bow_tsne_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_bow_tsne_clean, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF + t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       236\n",
      "           1       0.00      0.00      0.00       287\n",
      "           2       0.09      0.01      0.02       290\n",
      "           3       0.03      0.03      0.03       285\n",
      "           4       0.00      0.00      0.00       312\n",
      "           5       0.00      0.00      0.00       308\n",
      "           6       0.00      0.00      0.00       276\n",
      "           7       0.04      0.02      0.03       304\n",
      "           8       0.05      0.61      0.09       279\n",
      "           9       0.10      0.02      0.03       308\n",
      "          10       0.00      0.00      0.00       309\n",
      "          11       0.05      0.24      0.09       290\n",
      "          12       0.00      0.00      0.00       304\n",
      "          13       0.00      0.00      0.00       300\n",
      "          14       0.00      0.00      0.00       297\n",
      "          15       0.08      0.05      0.06       292\n",
      "          16       0.00      0.00      0.00       270\n",
      "          17       0.00      0.00      0.00       272\n",
      "          18       0.00      0.00      0.00       239\n",
      "          19       0.00      0.00      0.00       196\n",
      "\n",
      "    accuracy                           0.05      5654\n",
      "   macro avg       0.02      0.05      0.02      5654\n",
      "weighted avg       0.02      0.05      0.02      5654\n",
      "\n",
      "F1 score weighted: 0.01775875651378727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jcbar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Aplicar t-SNE sobre los datos TF-IDF ya limpios\n",
    "tsne = TSNE(n_components=2, random_state=42)  # Puedes cambiar a 3 si prefieres visualización en 3D\n",
    "X_train_tfidf_tsne_clean = tsne.fit_transform(X_train_tfidf_clean.toarray())  # Convertir a array para t-SNE\n",
    "X_test_tfidf_tsne_clean = tsne.fit_transform(X_test_tfidf_clean.toarray())\n",
    "\n",
    "# Entrenar el modelo de regresión logística con los datos reducidos por t-SNE\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_tsne_clean, y_train_clean)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_tfidf_tsne_clean = lr.predict(X_test_tfidf_tsne_clean)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_clean, y_pred_tfidf_tsne_clean))\n",
    "print(f\"F1 score weighted: {f1_score(y_test_clean, y_pred_tfidf_tsne_clean, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Escoge dos clases que crees que se diferencien muy bien entre sí con estos modelos. ¿Qué clases escogiste y por qué? Compara BOW y TF-IDF para la clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Listar las clases disponibles\n",
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos las clases *sci.electronics* y *talk.religion.misc*. Considero que la electrónica maneja un vocabulario muy específico que dificilmente coincidirá con el vocabulario empleado en la jerga religiosa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escoger dos clases: sci.electronics y talk.religion.misc\n",
    "categories = ['sci.electronics', 'talk.religion.misc']\n",
    "\n",
    "# Cargar el corpus con solo las dos clases seleccionadas, quitando headers, quotes, footers\n",
    "newsgroups_binary = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'quotes', 'footers'))\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X_binary = newsgroups_binary.data\n",
    "y_binary = newsgroups_binary.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary, y_binary, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       299\n",
      "           1       0.93      0.82      0.87       185\n",
      "\n",
      "    accuracy                           0.90       484\n",
      "   macro avg       0.91      0.89      0.90       484\n",
      "weighted avg       0.91      0.90      0.90       484\n",
      "\n",
      "F1 score (BOW): 0.867816091954023\n",
      "F1 score weighted (BOW): 0.9036407149664304\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score (BOW): {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted (BOW): {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94       299\n",
      "           1       0.99      0.79      0.88       185\n",
      "\n",
      "    accuracy                           0.92       484\n",
      "   macro avg       0.94      0.89      0.91       484\n",
      "weighted avg       0.92      0.92      0.91       484\n",
      "\n",
      "F1 score (TF-IDF): 0.8768768768768769\n",
      "F1 score weighted (TF-IDF): 0.9130510545685921\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones TF-IDF\n",
    "vectorizer_binary_tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf_binary = vectorizer_binary_tfidf.fit_transform(X_train_binary)\n",
    "X_test_tfidf_binary = vectorizer_binary_tfidf.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_tfidf_binary = lr.predict(X_test_tfidf_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(classification_report(y_test_binary, y_pred_tfidf_binary))\n",
    "print(f\"F1 score (TF-IDF): {f1_score(y_test_binary, y_pred_tfidf_binary)}\")\n",
    "print(f\"F1 score weighted (TF-IDF): {f1_score(y_test_binary, y_pred_tfidf_binary, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compara tu clasificador de la tarea pasada con el mejor clasificador del paso 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       159\n",
      "           1       0.63      0.93      0.75        83\n",
      "\n",
      "    accuracy                           0.79       242\n",
      "   macro avg       0.79      0.82      0.78       242\n",
      "weighted avg       0.84      0.79      0.79       242\n",
      "\n",
      "F1 Score: 0.7475728155339806\n",
      "F1 Score weighted: 0.7905270103102305\n"
     ]
    }
   ],
   "source": [
    "from sentiment_model import SentimentModel\n",
    "\n",
    "# 1. Cargar las clases 'sci.electronics' y 'talk.religion.misc' del corpus 20newsgroups\n",
    "categories = ['sci.electronics', 'talk.religion.misc']\n",
    "\n",
    "# Remover headers, footers y quotes\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Crear un DataFrame con los datos cargados\n",
    "df = pd.DataFrame({\n",
    "    'Text': newsgroups.data,\n",
    "    'Label': newsgroups.target  # target será 0 o 1 dependiendo de la categoría\n",
    "})\n",
    "\n",
    "# 2. Dividir los datos en conjunto de entrenamiento y prueba\n",
    "train_data, test_data = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3. Crear el modelo de SentimentModel\n",
    "model = SentimentModel()\n",
    "\n",
    "# 4. Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(train_data)\n",
    "\n",
    "# 5. Predecir la polaridad para los datos de prueba\n",
    "y_pred = model.predict(test_data)\n",
    "\n",
    "# 6. Evaluar el modelo utilizando classification_report y f1_score\n",
    "print(classification_report(test_data['Label'], y_pred))\n",
    "print(f\"F1 Score: {f1_score(test_data['Label'], y_pred)}\")\n",
    "print(f\"F1 Score weighted: {f1_score(test_data['Label'], y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Escoge ahora dos clases que crees que no se diferencien entre sí con estos modelos. ¿Qué clases escogiste y por qué? Compara BOW y TF-IDF para la clasificación binaria. ¿Qué tanto bajó el rendimiento respecto al paso 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos las clases *comp.sys.ibm.pc.hardware* y *comp.sys.mac.hardware*. Ambas clases tratan sobre hardware, la única diferencia es que están enfocadas en distintas plataformas, lo que podría dificultar la clasificación debido a que comparten mucha terminología técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escoger dos clases que podrían no diferenciarse bien: comp.sys.ibm.pc.hardware y comp.sys.mac.hardware\n",
    "categories = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
    "\n",
    "# Cargar el corpus con las dos clases seleccionadas\n",
    "newsgroups_similar = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'quotes', 'footers'))\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X_similar = newsgroups_similar.data\n",
    "y_similar = newsgroups_similar.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_similar, X_test_similar, y_train_similar, y_test_similar = train_test_split(X_similar, y_similar, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW (clases similares):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       155\n",
      "           1       0.79      0.84      0.82       137\n",
      "\n",
      "    accuracy                           0.82       292\n",
      "   macro avg       0.82      0.82      0.82       292\n",
      "weighted avg       0.82      0.82      0.82       292\n",
      "\n",
      "F1 score (BOW): 0.8156028368794326\n",
      "F1 score weighted (BOW): 0.8220850922281779\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_similar_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow_similar = vectorizer_similar_bow.fit_transform(X_train_similar)\n",
    "X_test_bow_similar = vectorizer_similar_bow.transform(X_test_similar)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr_similar_bow = LogisticRegression(max_iter=1000)\n",
    "lr_similar_bow.fit(X_train_bow_similar, y_train_similar)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_similar = lr_similar_bow.predict(X_test_bow_similar)\n",
    "\n",
    "# Evaluar el rendimiento del modelo BOW\n",
    "print(\"Clasificación BOW (clases similares):\\n\")\n",
    "print(classification_report(y_test_similar, y_pred_bow_similar))\n",
    "print(f\"F1 score (BOW): {f1_score(y_test_similar, y_pred_bow_similar)}\")\n",
    "print(f\"F1 score weighted (BOW): {f1_score(y_test_similar, y_pred_bow_similar, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación TF-IDF (clases similares):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83       155\n",
      "           1       0.80      0.83      0.82       137\n",
      "\n",
      "    accuracy                           0.83       292\n",
      "   macro avg       0.82      0.83      0.82       292\n",
      "weighted avg       0.83      0.83      0.83       292\n",
      "\n",
      "F1 score (TF-IDF): 0.8172043010752689\n",
      "F1 score weighted (TF-IDF): 0.8254758782891322\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones TF-IDF\n",
    "vectorizer_similar_tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf_similar = vectorizer_similar_tfidf.fit_transform(X_train_similar)\n",
    "X_test_tfidf_similar = vectorizer_similar_tfidf.transform(X_test_similar)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr_similar_tfidf = LogisticRegression(max_iter=1000)\n",
    "lr_similar_tfidf.fit(X_train_tfidf_similar, y_train_similar)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_tfidf_similar = lr_similar_tfidf.predict(X_test_tfidf_similar)\n",
    "\n",
    "# Evaluar el rendimiento del modelo TF-IDF\n",
    "print(\"Clasificación TF-IDF (clases similares):\\n\")\n",
    "print(classification_report(y_test_similar, y_pred_tfidf_similar))\n",
    "print(f\"F1 score (TF-IDF): {f1_score(y_test_similar, y_pred_tfidf_similar)}\")\n",
    "print(f\"F1 score weighted (TF-IDF): {f1_score(y_test_similar, y_pred_tfidf_similar, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En BOW\n",
      "\n",
      "El accuracy bajó 0.1\n",
      "El f1-score bajó 0.094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"En BOW\\n\")\n",
    "print(f\"El accuracy bajó {round(0.91 - 0.81, 3)}\")\n",
    "print(f\"El f1-score bajó {round(0.9076918657797048 - 0.8133150933317171, 3)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En TF-IDF\n",
      "\n",
      "El accuracy bajó 0.06\n",
      "El f1-score bajó 0.058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"En TF-IDF\\n\")\n",
    "print(f\"El accuracy bajó {round(0.90 - 0.84, 3)}\")\n",
    "print(f\"El f1-score bajó {round(0.9023588506528845 - 0.8441895050476068, 3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. En tu mejor clasificador del paso 3, prueba bajando y subiendo el parámetro `max_features` ¿qué efecto tiene esto en la tarea de clasificación?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW (max_features=100):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       299\n",
      "           1       0.88      0.73      0.80       185\n",
      "\n",
      "    accuracy                           0.86       484\n",
      "   macro avg       0.86      0.83      0.84       484\n",
      "weighted avg       0.86      0.86      0.85       484\n",
      "\n",
      "F1 score: 0.7964601769911505\n",
      "F1 score weighted: 0.854432749397493\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=100, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(\"Clasificación BOW (max_features=100):\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score: {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted: {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW (max_features=500):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       299\n",
      "           1       0.93      0.79      0.86       185\n",
      "\n",
      "    accuracy                           0.90       484\n",
      "   macro avg       0.91      0.88      0.89       484\n",
      "weighted avg       0.90      0.90      0.90       484\n",
      "\n",
      "F1 score: 0.8571428571428571\n",
      "F1 score weighted: 0.8969624557260921\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=500, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(\"Clasificación BOW (max_features=500):\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score: {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted: {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW (max_features=1000):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       299\n",
      "           1       0.93      0.82      0.87       185\n",
      "\n",
      "    accuracy                           0.90       484\n",
      "   macro avg       0.91      0.89      0.90       484\n",
      "weighted avg       0.91      0.90      0.90       484\n",
      "\n",
      "F1 score: 0.867816091954023\n",
      "F1 score weighted: 0.9036407149664304\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(\"Clasificación BOW (max_features=1000):\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score: {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted: {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW (max_features=2000):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       299\n",
      "           1       0.94      0.83      0.88       185\n",
      "\n",
      "    accuracy                           0.92       484\n",
      "   macro avg       0.92      0.90      0.91       484\n",
      "weighted avg       0.92      0.92      0.91       484\n",
      "\n",
      "F1 score: 0.8825214899713467\n",
      "F1 score weighted: 0.9141775872310337\n"
     ]
    }
   ],
   "source": [
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=2000, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "print(\"Clasificación BOW (max_features=2000):\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score: {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted: {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El incremento de max_features parece incrementar de manera asíntotica el rendimiento del modelo en términos de su f1-score. Notamos un claro incremento al pasar de 100 a 500 y luego de 500 a 1000. Sin embargo, al pasar de 1000 a 2000 el incremento es mínimo, casi imperceptible. Aunque el accuracy parece seguir el mismo comportamiento al pasar de 100 a 500 y de 500 a 1000, mostró un descenso al pasar de 1000 a 2000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Crédito extra**: ¿Qué efecto tiene lematizar el texto en la tarea de clasificación? Prueba con tu mejor clasificador binario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías para lematizar\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jcbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcbar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función para lematizar los documentos\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words if word.isalnum()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW sin lematización:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       299\n",
      "           1       0.94      0.82      0.87       185\n",
      "\n",
      "    accuracy                           0.91       484\n",
      "   macro avg       0.92      0.89      0.90       484\n",
      "weighted avg       0.91      0.91      0.91       484\n",
      "\n",
      "F1 score (BOW sin lematización): 0.8728323699421965\n",
      "F1 score weighted (BOW sin lematización): 0.9076918657797048\n"
     ]
    }
   ],
   "source": [
    "# Escoger dos clases: sci.electronics y talk.religion.misc\n",
    "categories = ['sci.electronics', 'talk.religion.misc']\n",
    "\n",
    "# Cargar el corpus con solo las dos clases seleccionadas, quitando headers, quotes, footers\n",
    "newsgroups_binary = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'quotes', 'footers'))\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X_binary = newsgroups_binary.data\n",
    "y_binary = newsgroups_binary.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_binary_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow_binary = vectorizer_binary_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_binary = vectorizer_binary_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_bow_binary, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_binary = lr.predict(X_test_bow_binary)\n",
    "\n",
    "# Evaluar el rendimiento del modelo BOW sin lematización\n",
    "print(\"Clasificación BOW sin lematización:\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_binary))\n",
    "print(f\"F1 score (BOW sin lematización): {f1_score(y_test_binary, y_pred_bow_binary)}\")\n",
    "print(f\"F1 score weighted (BOW sin lematización): {f1_score(y_test_binary, y_pred_bow_binary, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación BOW con lematización:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       299\n",
      "           1       0.93      0.82      0.87       185\n",
      "\n",
      "    accuracy                           0.90       484\n",
      "   macro avg       0.91      0.89      0.90       484\n",
      "weighted avg       0.91      0.90      0.90       484\n",
      "\n",
      "F1 score (BOW con lematización): 0.867816091954023\n",
      "F1 score weighted (BOW con lematización): 0.9036407149664304\n"
     ]
    }
   ],
   "source": [
    "# Escoger dos clases: sci.electronics y talk.religion.misc\n",
    "categories = ['sci.electronics', 'talk.religion.misc']\n",
    "\n",
    "# Cargar el corpus con las dos clases seleccionadas\n",
    "newsgroups_binary = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'quotes', 'footers'))\n",
    "\n",
    "# Aplicar lematización al texto\n",
    "X_binary_lemmatized = [lemmatize_text(text) for text in newsgroups_binary.data]\n",
    "y_binary = newsgroups_binary.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary_lemmatized, y_binary, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convertir los textos a representaciones BOW\n",
    "vectorizer_lemmatized_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow_lemmatized = vectorizer_lemmatized_bow.fit_transform(X_train_binary)\n",
    "X_test_bow_lemmatized = vectorizer_lemmatized_bow.transform(X_test_binary)\n",
    "\n",
    "# Entrenar el modelo de regresión logística\n",
    "lr_bow_lemmatized = LogisticRegression(max_iter=1000)\n",
    "lr_bow_lemmatized.fit(X_train_bow_lemmatized, y_train_binary)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_bow_lemmatized = lr_bow_lemmatized.predict(X_test_bow_lemmatized)\n",
    "\n",
    "# Evaluar el rendimiento del modelo BOW con lematización\n",
    "print(\"Clasificación BOW con lematización:\\n\")\n",
    "print(classification_report(y_test_binary, y_pred_bow_lemmatized))\n",
    "print(f\"F1 score (BOW con lematización): {f1_score(y_test_binary, y_pred_bow_lemmatized)}\")\n",
    "print(f\"F1 score weighted (BOW con lematización): {f1_score(y_test_binary, y_pred_bow_lemmatized, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La lematización parece reducir levemente el rendimiento del modelo BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy bajó 0.01\n",
      "El f1-score bajó 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"El accuracy bajó {round(0.91 - 0.90, 3)}\")\n",
    "print(f\"El f1-score bajó {round(0.9076918657797048 - 0.9036407149664304, 3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information Retrieval con el corpus `20newsgroups`** Entrena un modelo BOW y un TF-IDF con todos los documentos juntos de `train` y `test`. Realiza algunas consultas al modelo para obtener los documentos más relevantes para tu busqueda. Reporta algunos casos que creas interesantes y explica porque los consideras interesantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todo el corpus 20newsgroups (tanto train como test)\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Separar los datos en características y labels\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Entrenar el modelo BOW (Bag of Words)\n",
    "vectorizer_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_bow = vectorizer_bow.fit_transform(X)\n",
    "\n",
    "# Entrenar el modelo TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función para realizar consultas\n",
    "def retrieve_documents(query, vectorizer, X_corpus):\n",
    "    # Convertir la consulta en un vector\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    \n",
    "    # Calcular la similitud coseno entre la consulta y todos los documentos\n",
    "    cosine_similarities = cosine_similarity(query_vec, X_corpus).flatten()\n",
    "    \n",
    "    # Ordenar los documentos por similitud (de mayor a menor)\n",
    "    most_similar_docs = cosine_similarities.argsort()[::-1]\n",
    "    \n",
    "    # Retornar los índices de los documentos más similares\n",
    "    return most_similar_docs, cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una consulta de ejemplo\n",
    "query = \"space exploration and technology\"\n",
    "\n",
    "# Recuperar los documentos más relevantes usando BOW\n",
    "docs_bow, similarities_bow = retrieve_documents(query, vectorizer_bow, X_bow)\n",
    "\n",
    "# Recuperar los documentos más relevantes usando TF-IDF\n",
    "docs_tfidf, similarities_tfidf = retrieve_documents(query, vectorizer_tfidf, X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función modificada para mostrar la información solicitada\n",
    "def show_top_documents(docs, similarities, num_docs=5):\n",
    "    for i in range(num_docs):\n",
    "        doc_index = docs[i]\n",
    "        doc_text = newsgroups.data[doc_index]\n",
    "\n",
    "        # Extraer el 'Subject' del documento si está presente\n",
    "        subject_line = \"\"\n",
    "        for line in doc_text.split('\\n'):\n",
    "            if line.lower().startswith(\"subject:\"):\n",
    "                subject_line = line\n",
    "                break\n",
    "        \n",
    "        # Mostrar la información solicitada\n",
    "        print(f\"Documento {i + 1}:\")\n",
    "        print(f\"Índice del documento: {doc_index}\")\n",
    "        print(f\"{subject_line}\")\n",
    "        print(f\"Similitud coseno: {similarities[doc_index]:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados usando BOW:\n",
      "\n",
      "Documento 1:\n",
      "Índice del documento: 15147\n",
      "Subject: End of the Space Age\n",
      "Similitud coseno: 0.6729\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 2:\n",
      "Índice del documento: 10867\n",
      "Subject: Space FAQ 13/15 - Interest Groups & Publications\n",
      "Similitud coseno: 0.6292\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 3:\n",
      "Índice del documento: 5513\n",
      "Subject: Space FAQ 05/15 - References\n",
      "Similitud coseno: 0.5401\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 4:\n",
      "Índice del documento: 10571\n",
      "Subject: Space FAQ 02/15 - Network Resources\n",
      "Similitud coseno: 0.5161\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 5:\n",
      "Índice del documento: 16258\n",
      "Subject: Tsniimach Enterprise\n",
      "Similitud coseno: 0.5108\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los 5 documentos más relevantes usando BOW\n",
    "print(\"Resultados usando BOW:\\n\")\n",
    "show_top_documents(docs_bow, similarities_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados usando TF-IDF:\n",
      "\n",
      "Documento 1:\n",
      "Índice del documento: 15147\n",
      "Subject: End of the Space Age\n",
      "Similitud coseno: 0.6750\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 2:\n",
      "Índice del documento: 10867\n",
      "Subject: Space FAQ 13/15 - Interest Groups & Publications\n",
      "Similitud coseno: 0.6329\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 3:\n",
      "Índice del documento: 5513\n",
      "Subject: Space FAQ 05/15 - References\n",
      "Similitud coseno: 0.5283\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 4:\n",
      "Índice del documento: 10571\n",
      "Subject: Space FAQ 02/15 - Network Resources\n",
      "Similitud coseno: 0.5224\n",
      "--------------------------------------------------------------------------------\n",
      "Documento 5:\n",
      "Índice del documento: 16258\n",
      "Subject: Tsniimach Enterprise\n",
      "Similitud coseno: 0.5119\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los 5 documentos más relevantes usando TF-IDF\n",
    "print(\"\\nResultados usando TF-IDF:\\n\")\n",
    "show_top_documents(docs_tfidf, similarities_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análisis de sentimientos con BOW/TFIDF**. Usando el corpus de la tarea anterior (el de turismo), entrena un clasificador de Machine Learning con los embeddings BOW/TF-IDF, ¿mejora el rendimiento respecto al que presentaste en clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de turismo\n",
    "turismo_df = pd.read_csv('Datos/train_turismo.csv')\n",
    "\n",
    "# Crear una columna con el texto concatenado de 'Title' y 'Opinion'\n",
    "turismo_df['Text'] = turismo_df['Title'].astype(str) + \" \" + turismo_df['Opinion'].astype(str)\n",
    "turismo_df = turismo_df.dropna()\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = turismo_df['Text']\n",
    "y = turismo_df['Label']  # Asumiendo que 'Label' contiene la clasificación de sentimientos\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo BOW:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.51      0.57       122\n",
      "           1       0.89      0.94      0.91       541\n",
      "\n",
      "    accuracy                           0.86       663\n",
      "   macro avg       0.77      0.72      0.74       663\n",
      "weighted avg       0.85      0.86      0.85       663\n",
      "\n",
      "F1 score (BOW): 0.9141824751580849\n",
      "F1 score weighted (BOW): 0.8501513493684769\n",
      "\n",
      "Resultados del modelo TF-IDF:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.37      0.51       122\n",
      "           1       0.87      0.98      0.92       541\n",
      "\n",
      "    accuracy                           0.87       663\n",
      "   macro avg       0.84      0.67      0.71       663\n",
      "weighted avg       0.86      0.87      0.85       663\n",
      "\n",
      "F1 score (TF-IDF): 0.9233449477351916\n",
      "F1 score weighted (TF-IDF): 0.8464781447709888\n"
     ]
    }
   ],
   "source": [
    "# **BOW** - Convertir los textos a representaciones Bag of Words\n",
    "vectorizer_bow = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "# **TF-IDF** - Convertir los textos a representaciones TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "# Entrenar un modelo de regresión logística con **BOW**\n",
    "model_bow = LogisticRegression(max_iter=1000)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "\n",
    "# Entrenar un modelo de regresión logística con **TF-IDF**\n",
    "model_tfidf = LogisticRegression(max_iter=1000)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar los modelos\n",
    "print(\"Resultados del modelo BOW:\\n\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "print(f\"F1 score (BOW): {f1_score(y_test, y_pred_bow)}\")\n",
    "print(f\"F1 score weighted (BOW): {f1_score(y_test, y_pred_bow, average='weighted')}\")\n",
    "\n",
    "print(\"\\nResultados del modelo TF-IDF:\\n\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "print(f\"F1 score (TF-IDF): {f1_score(y_test, y_pred_tfidf)}\")\n",
    "print(f\"F1 score weighted (TF-IDF): {f1_score(y_test, y_pred_tfidf, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.48      0.58       122\n",
      "           1       0.89      0.96      0.92       541\n",
      "\n",
      "    accuracy                           0.87       663\n",
      "   macro avg       0.80      0.72      0.75       663\n",
      "weighted avg       0.86      0.87      0.86       663\n",
      "\n",
      "F1 Score: 0.9223907225691347\n",
      "F1 Score weighted: 0.858578840352646\n"
     ]
    }
   ],
   "source": [
    "# Importar el modelo de análisis de sentimientos\n",
    "from sentiment_model import SentimentModel\n",
    "\n",
    "# Crear el modelo de SentimentModel con los parámetros deseados\n",
    "model = SentimentModel()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo utilizando classification_report y f1_score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score weighted: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
